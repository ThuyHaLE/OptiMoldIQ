from loguru import logger
from dataclasses import dataclass, field
import shutil
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional, Literal
from pathlib import Path
from agents.utils import read_change_log
from configs.recovery.dataPipelineOrchestrator.data_pipeline_orchestrator_configs import AgentExecutionInfo
from agents.autoPlanner.initialPlanner.compatibility_based_mold_machine_optimizer import PriorityOrder
from agents.autoPlanner.reportFormatters.dict_based_report_generator import DictBasedReportGenerator

@dataclass
class WorkflowConfig:
    """Configuration class for workflow parameters"""
    db_dir: str = 'database'
    dynamic_db_dir: str = 'database/dynamicDatabase'
    shared_db_dir: str = 'agents/shared_db'
    efficiency: float = 0.85
    loss: float = 0.03
    historical_insight_threshold: int = 30

    # PendingProcessor
    max_load_threshold: int = 30
    priority_order: PriorityOrder = PriorityOrder.PRIORITY_1
    verbose: bool = True
    use_sample_data: bool = False

    # MoldStabilityIndexCalculator
    cavity_stability_threshold: float = 0.6
    cycle_stability_threshold: float = 0.4
    total_records_threshold: int = 30

    # MoldMachineFeatureWeightCalculator
    scaling: Literal['absolute', 'relative'] = 'absolute'
    confidence_weight: float = 0.3
    n_bootstrap: int = 500
    confidence_level: float = 0.95
    min_sample_size: int = 10
    feature_weights: Optional[Dict[str, float]] = None
    targets: dict = field(default_factory=lambda: {
        'shiftNGRate': 'minimize',
        'shiftCavityRate': 1.0,
        'shiftCycleTimeRate': 1.0,
        'shiftCapacityRate': 1.0
    })


@dataclass
class WorkflowConstants:
    """Constants used throughout the workflow"""
    FULL_SEPARATOR: str = "=" * 80
    PARTIAL_SEPARATOR: str = "-" * 60
    MEDIUM_SEPARATOR: str = "=" * 35
    SHORT_SEPARATOR: str = "=" * 30

    # File names
    ANNOTATION_FILE: str = 'path_annotations.json'
    CHANGE_LOG_FILE: str = 'change_log.txt'
    DATABASE_SCHEMAS_FILE: str = 'databaseSchemas.json'
    SHARED_DATABASE_SCHEMAS_FILE: str = 'sharedDatabaseSchemas.json'
    WEIGHTS_HIST_FILE: str = 'weights_hist.xlsx'

    # Report sections
    DATA_COLLECTION_HEADER: str = "GENERATED DATA PROCESSING REPORTS"

    SHARED_DB_HEADER_SECTION: str = "RESULTS OF CHANGE DETECTION IN PURCHASE ORDERS"
    SHARED_DB_HEADER: str = "GENERATED SHARED DATABASE BUILDING REPORTS"
    VALIDATION_ORCHESTRATOR_HEADER: str = 'VALIDATION ORCHESTRATOR REPORT'
    ORDER_PROGRESS_TRACKER_HEADER: str = "ORDER PROGRESS TRACKER REPORT"
    PRODUCING_PROGRESS_HEADER: str = "INITIAL PLANNER (PRODUCING PROGRESS) REPORT"

    SUFFICIENT_DATE_RECORDS_SECTION: str = "RESULTS OF DETECTION IN SUFFICIENT DATE RECORDS"
    HISTORICAL_INSIGHTS_HEADER: str = "GENERATED HISTORICAL INSIGHTS REPORTS"
    MOLD_STABILITY_INDEX_CALCULATOR_HEADER: str = "GENERATED MOLD STABILITY INDEX REPORTS"
    MOLD_MACHINE_FEATURE_WEIGHT_CALCULATOR_HEADER: str = "GENERATED MOLD MACHINE FEATURE WEIGHT REPORTS"

    PURCHASE_ORDER_SECTION: str = "RESULTS OF CHANGE DETECTION IN PURCHASE ORDERS"
    PENDING_PROGRESS_HEADER: str = "INITIAL PLANNER (PENDING PROGRESS) REPORT"

    PROGRESS_OUTPUT_COLLECTION_HEADER: str = "PROGRESS_OUTPUT_COLLECTION"

    FOOTER_MESSAGE: str = "This report was automatically generated by the OptiMoldIQ Workflow system."

class WorkflowError(Exception):
    """Custom exception for workflow-related errors"""
    pass


class PathManager:
    """Manages file paths for the workflow"""

    def __init__(self, config: WorkflowConfig):
        self.config = config
        self._base_paths = {
            'db_dir': Path(config.db_dir),
            'dynamic_db_dir': Path(config.dynamic_db_dir),
            'shared_db_dir': Path(config.shared_db_dir)
        }

    def get_database_schemas_path(self) -> str:
        return str(self._base_paths['db_dir'] / WorkflowConstants.DATABASE_SCHEMAS_FILE)

    def get_shared_database_schemas_path(self) -> str:
        return str(self._base_paths['db_dir'] / WorkflowConstants.SHARED_DATABASE_SCHEMAS_FILE)

    def get_data_loader_path(self) -> str:
        return str(self._base_paths['shared_db_dir'] / "DataLoaderAgent" / "newest")

    def get_annotation_path(self) -> str:
        return str(Path(self.get_data_loader_path()) / WorkflowConstants.ANNOTATION_FILE)

    def get_data_pipeline_orchestrator_path(self) -> str:
        return str(self._base_paths['shared_db_dir'] / "DataPipelineOrchestrator")

    def get_validation_orchestrator_path(self) -> str:
        return str(self._base_paths['shared_db_dir'] / "ValidationOrchestrator")

    def get_order_progress_tracker_path(self) -> str:
        return str(self._base_paths['shared_db_dir'] / "OrderProgressTracker")

    def get_producing_processor_path(self) -> str:
        return str(self._base_paths['shared_db_dir'] / "ProducingProcessor")

    def get_pending_processor_path(self) -> str:
        return str(self._base_paths['shared_db_dir'] / "PendingProcessor")

    def get_mold_stability_index_path(self) -> str:
        return str(self._base_paths['shared_db_dir'] / "MoldStabilityIndexCalculator")

    def get_mold_machine_weights_path(self) -> str:
        return str(self._base_paths['shared_db_dir'] / "MoldMachineFeatureWeightCalculator" / WorkflowConstants.WEIGHTS_HIST_FILE)


class ReportManager:
    """Manages report generation and formatting"""

    def __init__(self, output_dir, filename_prefix):
        self.logger = logger.bind(class_="ReportManager")
        self.report_lines: List[str] = ['\n']
        self.output_dir = output_dir
        self.filename_prefix = filename_prefix

    def add_header(self, title: str, separator_type: str = "full") -> None:
        """Add a formatted header to the report"""
        if separator_type == "full":
            separator = WorkflowConstants.FULL_SEPARATOR
        elif separator_type == "partial":
            separator = WorkflowConstants.PARTIAL_SEPARATOR
        elif separator_type == "medium":
            separator = WorkflowConstants.MEDIUM_SEPARATOR
        else:
            separator = WorkflowConstants.SHORT_SEPARATOR

        self.report_lines.extend([separator, title, separator, ""])

    def add_section(self, content: List[str]) -> None:
        """Add a section to the report"""
        self.report_lines.extend(content)

    def add_message(self, message: str) -> None:
        """Add a single message to the report"""
        self.report_lines.append(message)

    def add_details_list(self, details: List[str], prefix: str = "Details: ") -> None:
        """Add a list of details to the report"""
        if details:
            self.report_lines.append(prefix)
            self.report_lines.extend([f"  - {detail}" for detail in details])

    def get_report_content(self) -> str:
        """Get the complete report as a string"""
        return "\n".join(self.report_lines)

    def save_report(self,
                    filename: Optional[str] = None
                    ) -> str:

        """Save the report to a file and return the filename"""

        output_dir = Path(self.output_dir)
        log_path = output_dir / "change_log.txt"
        timestamp_now = datetime.now()
        timestamp_str = timestamp_now.strftime("%Y-%m-%d %H:%M:%S")
        log_entries = [f"[{timestamp_str}] Saving new version...\n"]

        newest_dir = output_dir / "newest"
        newest_dir.mkdir(parents=True, exist_ok=True)
        historical_dir = output_dir / "historical_db"
        historical_dir.mkdir(parents=True, exist_ok=True)

        # Move old files to historical_db
        for f in newest_dir.iterdir():
            if f.is_file():
                try:
                    dest = historical_dir / f.name
                    shutil.move(str(f), dest)
                    log_entries.append(f"  ⤷ Moved old file: {f.name} → historical_db/{f.name}\n")
                    self.logger.info("Moved old file {} to historical_db as {}", f.name, dest.name)
                except Exception as e:
                    self.logger.error("Failed to move file {}: {}", f.name, e)
                    raise OSError(f"Failed to move file {f.name}: {e}")

        if filename is None:
            timestamp_file = timestamp_now.strftime("%Y%m%d_%H%M")
            filename = f"{timestamp_file}_{self.filename_prefix}_report.txt"
            new_path = newest_dir / filename

        # Add footer
        self.report_lines.extend([
            "\n\n",
            f"*** {WorkflowConstants.FOOTER_MESSAGE} ***"
        ])

        try:
            with open(new_path, 'w', encoding='utf-8') as f:
                f.write(self.get_report_content())
            log_entries.append(f"  ⤷ Saved new file: {newest_dir}/{filename}\n")
            self.logger.info("Saved new file: {}/{}", newest_dir, filename)
        except Exception as e:
            self.logger.error("Failed to save file {}: {}", filename, e)
            raise OSError(f"Failed to save file {filename}: {e}")

        try:
            with open(log_path, "a", encoding="utf-8") as log_file:
                log_file.writelines(log_entries)
            self.logger.info("Updated change log {}", log_path)
        except Exception as e:
            self.logger.error("Failed to update change log {}: {}", log_path, e)
            raise OSError(f"Failed to update change log {log_path}: {e}")

        return filename

class OptiMoldIQWorkflow:
    """Orchestrates the daily data pipeline workflow"""

    def __init__(self,
                 config: WorkflowConfig):

        self.logger = logger.bind(class_="OptiMoldIQWorkflow")
        self.config = config
        self.path_manager = PathManager(config)
        self.report_manager = ReportManager(Path(self.config.shared_db_dir) / "OptiMoldIQWorkflow",
                                            "OptiMoldIQWorkflow")
        self.dict_based_reporter = DictBasedReportGenerator(use_colors=False)
        self.output_path_collection = {}

    def _safe_execute(self,
                      operation_name: str,
                      operation_func,
                      *args, **kwargs) -> Any:
        """
        Safely execute an operation with consistent error handling and logging

        Args:
            operation_name: Name of the operation for logging
            operation_func: Function to execute
            *args, **kwargs: Arguments to pass to the function

        Returns:
            Result of the operation

        Raises:
            WorkflowError: If the operation fails
        """
        try:
            self.logger.info(f"Starting {operation_name}...")
            result = operation_func(*args, **kwargs)
            self.logger.info(f"{operation_name} completed successfully")
            return result
        except Exception as e:
            self.logger.error(f"Error in {operation_name}: {e}")
            raise WorkflowError(f"Failed to execute {operation_name}: {e}") from e

    def get_updated_output_path(self,
                                folder_path,
                                target_name):
        return read_change_log(folder_path, target_name)

    def detect_updates(self,
                       data_pipeline_report) -> Tuple[bool, List[str], bool]:
        """
        Detect updates in the data pipeline report.

        Args:
            data_pipeline_report: Report from DataPipelineOrchestrator

        Returns:
            Tuple of (trigger_flag, list_of_updated_details)
        """
        updated_db_details = []

        # Check collector results
        for db in data_pipeline_report['collector_result'].details:
            if db.get('data_updated', False):
                updated_db_details.append(db['data_type'])

        # Check loader results (excluding last item)
        for db in data_pipeline_report['loader_result'].details[:-1]:
            if db.get('data_updated', False):
                updated_db_details.append(db['database_name'])

        trigger = len(updated_db_details) > 0

        # Check if need collect historical insight
        def check_historical_insight_request(df,
                                             historical_insight_threshold=30):
            num_of_record_date = len(df['recordDate'].unique())
            return num_of_record_date % historical_insight_threshold == 0

        historical_insight_request = False
        for db in data_pipeline_report['loader_result'].details:  # Fixed variable name
            if db['database_name'] == 'productRecords':
                productRecords_df = db['dataframe']
                historical_insight_request = check_historical_insight_request(
                    productRecords_df,
                    self.config.historical_insight_threshold
                )
                break  # Exit loop once we find productRecords

        return trigger, updated_db_details, historical_insight_request

    def _run_data_collection(self) -> AgentExecutionInfo:
        """Run the data collection phase of the workflow"""

        def _execute_data_collection():
            # Lazy import for conditional execution
            from agents.dataPipelineOrchestrator.data_pipeline_orchestrator import DataPipelineOrchestrator

            orchestrator = DataPipelineOrchestrator(
                dynamic_db_source_dir=self.config.dynamic_db_dir,
                databaseSchemas_path=self.path_manager.get_database_schemas_path(),
                annotation_path=self.path_manager.get_annotation_path(),
                default_dir=self.config.shared_db_dir
            )

            return orchestrator.run_pipeline()

        return self._safe_execute("DataPipelineOrchestrator", _execute_data_collection)

    def _run_validation_orchestrator(self):
        """Run the ValidationOrchestrator"""

        def _execute_validation():
            # Lazy import
            from agents.validationOrchestrator.validation_orchestrator import ValidationOrchestrator

            validator = ValidationOrchestrator(
                source_path=self.path_manager.get_data_loader_path(),
                annotation_name=WorkflowConstants.ANNOTATION_FILE,
                databaseSchemas_path=self.path_manager.get_database_schemas_path(),
                default_dir=self.config.shared_db_dir
            )

            return validator.run_validations_and_save_results()

        results = self._safe_execute("ValidationOrchestrator", _execute_validation)

        self.report_manager.add_header(WorkflowConstants.VALIDATION_ORCHESTRATOR_HEADER)
        self.report_manager.add_section(self.dict_based_reporter.export_report(results))

    def _run_order_progress_tracker(self):
        """Run the OrderProgressTracker"""

        def _execute_tracking():
            # Lazy import
            from agents.orderProgressTracker.order_progress_tracker import OrderProgressTracker

            tracker = OrderProgressTracker(
                source_path=self.path_manager.get_data_loader_path(),
                annotation_name=WorkflowConstants.ANNOTATION_FILE,
                databaseSchemas_path=self.path_manager.get_database_schemas_path(),
                folder_path=self.path_manager.get_validation_orchestrator_path(),
                target_name=WorkflowConstants.CHANGE_LOG_FILE,
                default_dir=self.config.shared_db_dir
            )

            return tracker.pro_status()

        results = self._safe_execute("OrderProgressTracker", _execute_tracking)

        self.report_manager.add_header(WorkflowConstants.ORDER_PROGRESS_TRACKER_HEADER)
        self.report_manager.add_section(self.dict_based_reporter.export_report(results))

    def _run_mold_stability_index_calculator(self):
        """Run the MoldStabilityIndexCalculator"""

        def _execute_index_calculating():
            # Lazy import
            from agents.autoPlanner.initialPlanner.historyBasedProcessor.mold_stability_index_calculator import MoldStabilityIndexCalculator
            calculator = MoldStabilityIndexCalculator(
                source_path = self.path_manager.get_data_loader_path(),
                annotation_name = WorkflowConstants.ANNOTATION_FILE,
                databaseSchemas_path = self.path_manager.get_database_schemas_path(),
                default_dir = self.config.shared_db_dir,
                efficiency = self.config.efficiency,
                loss = self.config.loss
            )
            return calculator.process_and_save_result(
                cavity_stability_threshold = self.config.cavity_stability_threshold,
                cycle_stability_threshold = self.config.cycle_stability_threshold,
                total_records_threshold = self.config.total_records_threshold)

        self._safe_execute("MoldStabilityIndexCalculator", _execute_index_calculating)

        self.report_manager.add_header(WorkflowConstants.MOLD_STABILITY_INDEX_CALCULATOR_HEADER)

        self.report_manager.add_section(["==> MoldStabilityIndexCalculator execution completed.\n"])

    def _run_mold_machine_feature_weight_calculator(self):
        """Run the MoldStabilityIndexCalculator"""

        def _execute_weight_calculating():
            # Lazy import
            from agents.autoPlanner.initialPlanner.historyBasedProcessor.mold_machine_feature_weight_calculator import MoldMachineFeatureWeightCalculator

            calculator = MoldMachineFeatureWeightCalculator(
                source_path = self.path_manager.get_data_loader_path(),
                annotation_name = WorkflowConstants.ANNOTATION_FILE,
                databaseSchemas_path = self.path_manager.get_database_schemas_path(),
                sharedDatabaseSchemas_path = self.path_manager.get_shared_database_schemas_path(),
                folder_path = self.path_manager.get_order_progress_tracker_path(),
                target_name = WorkflowConstants.CHANGE_LOG_FILE,
                default_dir = self.config.shared_db_dir,
                efficiency = self.config.efficiency,
                loss = self.config.loss,
                scaling = self.config.scaling,
                confidence_weight = self.config.confidence_weight,
                n_bootstrap = self.config.n_bootstrap,
                confidence_level = self.config.confidence_level,
                min_sample_size = self.config.min_sample_size,
                feature_weights = self.config.feature_weights,
                targets = self.config.targets
                )

            return calculator.calculate_and_save_report(
                mold_stability_index_folder=self.path_manager.get_mold_stability_index_path(),
                mold_stability_index_target_name=WorkflowConstants.CHANGE_LOG_FILE,
            )

        self._safe_execute("MoldMachineFeatureWeightCalculator", _execute_weight_calculating)

        self.report_manager.add_header(WorkflowConstants.MOLD_MACHINE_FEATURE_WEIGHT_CALCULATOR_HEADER)

        self.report_manager.add_section(["==> MoldMachineFeatureWeightCalculator execution completed.\n"])

    def _run_producing_processor(self):
        """Run the ProducingProcessor"""

        def _execute_producing():
            # Lazy import
            from agents.autoPlanner.initialPlanner.producing_processor import ProducingProcessor

            processor = ProducingProcessor(
                source_path=self.path_manager.get_data_loader_path(),
                annotation_name=WorkflowConstants.ANNOTATION_FILE,
                databaseSchemas_path=self.path_manager.get_database_schemas_path(),
                sharedDatabaseSchemas_path=self.path_manager.get_shared_database_schemas_path(),
                folder_path=self.path_manager.get_order_progress_tracker_path(),
                target_name=WorkflowConstants.CHANGE_LOG_FILE,
                mold_stability_index_folder=self.path_manager.get_mold_stability_index_path(),
                mold_stability_index_target_name=WorkflowConstants.CHANGE_LOG_FILE,
                mold_machine_weights_hist_path=self.path_manager.get_mold_machine_weights_path(),
                default_dir=self.config.shared_db_dir,
                efficiency=self.config.efficiency,
                loss=self.config.loss
            )

            return processor.process_and_save_results()

        results = self._safe_execute("ProducingProcessor", _execute_producing)

        self.report_manager.add_header(WorkflowConstants.PRODUCING_PROGRESS_HEADER)
        self.report_manager.add_section(self.dict_based_reporter.export_report(results))

    def _run_pending_processor(self):
        """Run the PendingProcessor"""

        def _execute_pending():
            # Lazy import
            from agents.autoPlanner.initialPlanner.pending_processor import PendingProcessor, ProcessingConfig

            config = ProcessingConfig(
                max_load_threshold=self.config.max_load_threshold,
                priority_order=self.config.priority_order,
                verbose=self.config.verbose,
                use_sample_data=self.config.use_sample_data
            )

            pending_processor = PendingProcessor(
                source_path=self.path_manager.get_data_loader_path(),
                annotation_name=WorkflowConstants.ANNOTATION_FILE,
                databaseSchemas_path=self.path_manager.get_database_schemas_path(),
                sharedDatabaseSchemas_path=self.path_manager.get_shared_database_schemas_path(),
                default_dir=self.config.shared_db_dir,
                producing_processor_folder_path=self.path_manager.get_producing_processor_path(),
                producing_processor_target_name=WorkflowConstants.CHANGE_LOG_FILE,
                config=config
            )

            return pending_processor.run_and_save_results()

        results = self._safe_execute("PendingProcessor", _execute_pending)

        self.report_manager.add_header(WorkflowConstants.PENDING_PROGRESS_HEADER, 'medium')
        self.report_manager.add_section(self.dict_based_reporter.export_report(results))


    def _execute_data_collection_phase(self) -> AgentExecutionInfo:
        """Execute the data collection phase"""
        self.report_manager.add_header(WorkflowConstants.DATA_COLLECTION_HEADER, "full")

        data_pipeline_report = self._run_data_collection()

        # Add data pipeline report to the main report
        pipeline_report_content = self.dict_based_reporter.export_report(data_pipeline_report)
        self.report_manager.add_section(pipeline_report_content)

        return data_pipeline_report

    def _execute_historical_insight_adding_phase(self,
                                                 historical_insight_request: bool):
        """Historical insights generating (optional when sufficient date records detected.)"""

        self.report_manager.add_header(WorkflowConstants.SUFFICIENT_DATE_RECORDS_SECTION, "full")

        if historical_insight_request:
            self.report_manager.add_message(
                "Date records are sufficient for historical insights. => Proceeding to generate insights...")

            self.logger.info(
                "Sufficient date records detected. Starting MoldStabilityIndexCalculator and MoldMachinePriorityMatrixCalculator")

            self.report_manager.add_header(WorkflowConstants.HISTORICAL_INSIGHTS_HEADER, "medium")

            self._run_mold_stability_index_calculator()
            self._run_mold_machine_feature_weight_calculator()

        else:
            self.report_manager.add_message(
                "Date records are not sufficient for historical insights. => Skipping historical insight adding phase!!!")
            self.logger.info(
                "Date records are not sufficient for historical insights. Skipping historical insight adding phase")

    def _execute_shared_db_building_phase(self,
                                          trigger: bool,
                                          updated_db_details: List[str],
                                          historical_insight_request: bool):
        """Execute the shared database building phase"""

        self.report_manager.add_header(WorkflowConstants.SHARED_DB_HEADER_SECTION, "full")

        if trigger:
            self.report_manager.add_message("Changes detected in databases => Proceeding to update shared databases...")
            self.report_manager.add_details_list(updated_db_details)

            self.logger.info("Changes detected. Starting ValidationOrchestrator and OrderProgressTracker")

            self.report_manager.add_header(WorkflowConstants.SHARED_DB_HEADER, "medium")

            self._run_validation_orchestrator()
            self._run_order_progress_tracker()
            self._execute_historical_insight_adding_phase(historical_insight_request)
            self._run_producing_processor()
        else:
            self.report_manager.add_message("No changes detected in databases => Skipping validation, tracking processes, and related information extraction!!!")
            self.logger.info("No changes detected. Skipping validation, tracking processes, and related information extraction")

    def _execute_initial_planner_phase(self,
                                       updated_db_details: List[str]):
        """Execute the initial planner phase"""

        self.report_manager.add_header(WorkflowConstants.PURCHASE_ORDER_SECTION, "full")

        if 'purchaseOrders' in updated_db_details:
            self.report_manager.add_message("Changes detected in purchaseOrders => Proceeding with initial planner...")

            self.logger.info('Purchase orders changes detected. Starting PendingProcessor')

            self._run_pending_processor()
        else:
            self.report_manager.add_message("No changes detected in purchaseOrders => Skipping PendingProcessor!!!")
            self.logger.info("No purchase order changes detected. Skipping PendingProcessor")

    def run_workflow(self) -> Dict[str, Any]:
        """
        Execute the complete daily workflow.

        Returns:
            Dictionary containing workflow results and report information
        """
        try:
            # Phase 1: Data Collection
            data_pipeline_report = self._execute_data_collection_phase()

            # Detect updates
            trigger, updated_db_details, historical_insight_request = self.detect_updates(data_pipeline_report)

            # Phase 2: Shared DB Building
            self._execute_shared_db_building_phase(trigger, updated_db_details, historical_insight_request)

            # Phase 3: Initial Planner
            self._execute_initial_planner_phase(updated_db_details)

            if trigger:
                self.output_path_collection['data_pipeline_orchestrator_path'] = "\n".join(self.get_updated_output_path(
                        self.path_manager.get_data_pipeline_orchestrator_path(),
                        WorkflowConstants.CHANGE_LOG_FILE))
                self.output_path_collection['order_progress_path'] = self.get_updated_output_path(
                        self.path_manager.get_order_progress_tracker_path(),
                        WorkflowConstants.CHANGE_LOG_FILE)
                self.output_path_collection['producing_plan_path'] = self.get_updated_output_path(
                        self.path_manager.get_producing_processor_path(),
                        WorkflowConstants.CHANGE_LOG_FILE)
                self.output_path_collection['pending_initial_plan_path'] = self.get_updated_output_path(
                        self.path_manager.get_pending_processor_path(),
                        WorkflowConstants.CHANGE_LOG_FILE)

                if historical_insight_request:
                    self.output_path_collection['mold_stability_index_path'] = self.get_updated_output_path(
                        self.path_manager.get_mold_stability_index_path(),
                        WorkflowConstants.CHANGE_LOG_FILE)
                    self.output_path_collection['mold_machine_weights_hist_path'] = self.path_manager.get_mold_machine_weights_path()

                self.report_manager.add_header(WorkflowConstants.PROGRESS_OUTPUT_COLLECTION_HEADER, "full")
                self.report_manager.add_section([f"{k}: {v}" for k, v in self.output_path_collection.items()])

            # Save the report and get filename
            report_filename = self.report_manager.save_report()

            self.logger.info("Daily workflow completed successfully")
            self.logger.info(f"Report saved as: {report_filename}")

            return {
                'status': 'success',
                'data_pipeline_report': data_pipeline_report,
                'trigger': trigger,
                'updated_db_details': updated_db_details,
                'report_filename': report_filename,
                'progress_output_collection': self.output_path_collection,
                'report_content': self.report_manager.get_report_content()
                }

        except WorkflowError:
            # Re-raise workflow errors as-is
            raise
        except Exception as e:
            self.logger.error(f"Critical error in daily workflow: {e}")
            raise WorkflowError(f"Workflow execution failed: {e}") from e