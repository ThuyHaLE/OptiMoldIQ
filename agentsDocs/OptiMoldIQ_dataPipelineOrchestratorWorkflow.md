## Workflow: DataPipelineOrchestrator

```

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                             [ DataPipelineOrchestrator ]                                        â”‚
â”‚                    Orchestrates the entire batch data processing flow                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–¼ Phase 1                                                           â–¼ Phase 2
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   DataCollector      â”‚                                            â”‚   DataLoaderAgent    â”‚
        â”‚ (Process Dynamic DB) â”‚                                            â”‚ (Load All Databases) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                                                                        â”‚
               â”‚                                                                        â–¼
               â”‚                                                           Check for changes in content of each database
               â–¼                                                                        â–¼
    1. Read report files `.xlsb`/`.xlsx`                                                If data has changed:
    2. Merge and process based on `monthlyReports_` and `purchaseOrder_`                     1. Compare with schema from `databaseSchemas.json`
    3. Validate missing fields, normalize datatypes                                          2. Compare new content with `path_annotations.json`
    4. Compare with existing parquet using hash                                              3. If different â†’ Save new version
    5. If changed â†’ save as new `.parquet` file                                              4. Log changes + update path_annotation.json                                                                 

```

---

### ğŸ” Details of Each Phase

#### Phase 1: `DataCollector`

| Step | Description                                                                  |
| ---- | ---------------------------------------------------------------------------- |
| 1    | Read data from `monthlyReports_history` and `purchaseOrders_history` folders |
| 2    | Handle missing columns, normalize date format, shifts, resin codes, etc.     |
| 3    | Remove duplicated data                                                       |
| 4    | Compare new data with existing `.parquet` using hash                         |
| 5    | If changed â†’ save new `.parquet` using atomic write and snappy compression   |

---

#### Phase 2: `DataLoaderAgent`

| Step | Description                                                                                                                                   |
| ---- | --------------------------------------------------------------------------------------------------------------------------------------------- |
| 1    | Load `databaseSchemas.json` to get schema definitions                                                                                         |
| 2    | Load `path_annotations.json` for existing version references                                                                                  |
| 3    | For each database:<br>- If `dynamicDB` â†’ load `.parquet` from Phase 1<br>- If `staticDB` â†’ load `.xlsx` from path                             |
| 4    | Compare new and old content using hash                                                                                                        |
| 5    | If different:<br>â†’ Move old file to `historical_db`<br>â†’ Save new file to `newest` with timestamp<br>â†’ Log and update `path_annotations.json` |

---

### Input/Output Overview

| Agent             | Input                                          | Output                                                 |
| ----------------- | ---------------------------------------------- | ------------------------------------------------------ |
| `DataCollector`   | Folders with `.xlsb`, `.xlsx` reports          | `productRecords.parquet`, `purchaseOrders.parquet`     |
| `DataLoaderAgent` | `.parquet` from Phase 1 + static `.xlsx` files | Versioned `.parquet` + updated `path_annotations.json` |
