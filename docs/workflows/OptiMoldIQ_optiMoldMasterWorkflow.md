# OptiMoldIQ Workflow Process

## Workflow Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                               [ OptiMoldIQWorkflow ]                                            â”‚
â”‚                    Main orchestrator coordinating all manufacturing workflow phases              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–¼ PHASE 1: DATA COLLECTION                                           
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ DataPipelineOrch.    â”‚                                            â”‚   Update Detection   â”‚
        â”‚ (Collect & Process)  â”‚â”€â”€â”€â”€â”€â”€ Process Pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â¯ˆâ”‚ (Analyze Changes)    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                                                                        â”‚
               â–¼                                                                        â–¼
    ðŸ“Š Execute Data Collection                                             ðŸ” Detect Database Updates
    â€¢ Run DataPipelineOrchestrator                                         â€¢ Check collector results
    â€¢ Process dynamic databases                                            â€¢ Check loader results  
    â€¢ Generate pipeline report                                             â€¢ Identify changed databases
    â€¢ Handle collection errors                                             â€¢ Return trigger flag & details

               â–¼ PHASE 2: SHARED DB BUILDING (Conditional)
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ ValidationOrch.      â”‚      â”‚ OrderProgressTracker â”‚      â”‚ Historical insight   â”‚      â”‚ ProducingProcessor   â”‚
        â”‚ (Data Validation)    â”‚â”€â”€â”€â”€â¯ˆâ”‚ (Progress Monitoring)â”‚â”€â”€â”€â”€â¯ˆ â”‚ adding phase         â”‚â”€â”€â”€â”€â¯ˆâ”‚ (Production Analysis)â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                              â”‚                              â”‚                                â”‚
               â–¼                              â–¼                              â–¼                                â–¼
    âœ… Validate Data Quality          ðŸ“ˆ Track Order Status       ðŸ“ˆ Generate Historical Insights   ðŸ­ Process Production Data
    â€¢ Run validation checks            â€¢ Monitor order progress     â€¢ Calculate:                      â€¢ Analyze production metrics
    â€¢ Generate mismatch reports        â€¢ Track milestones           1. mold stability index           â€¢ Calculate efficiency & loss
    â€¢ Ensure data integrity            â€¢ Update progress logs       2. mold machine feature weight    â€¢ Generate production reports
    â€¢ Save validation results          â€¢ Generate progress reports                                    â€¢ Process stability indices

               â–¼ PHASE 3: INITIAL PLANNING (Conditional)
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Purchase Order     â”‚                                            â”‚   PendingProcessor   â”‚
        â”‚   Change Detection   â”‚â”€â”€â”€â”€â”€â”€ If PO Changes Detected â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â¯ˆâ”‚ (Order Processing)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                                                                        â”‚
               â–¼                                                                        â–¼
    ðŸ›’ Check Purchase Orders                                           âš¡ Process Pending Orders
    â€¢ Analyze updated databases                                        â€¢ Apply priority ordering
    â€¢ Look for 'purchaseOrders' changes                               â€¢ Respect load thresholds
    â€¢ Determine if planning needed                                     â€¢ Optimize processing schedule
    â€¢ Trigger or skip processing                                       â€¢ Generate planning reports

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                ðŸ“‹ REPORTING SYSTEM                                  â”‚
        â”‚  â€¢ Generate comprehensive workflow reports                                          â”‚
        â”‚  â€¢ Include data collection, validation, progress, and planning results              â”‚
        â”‚  â€¢ Save timestamped reports with UTF-8 encoding                                     â”‚
        â”‚  â€¢ Provide audit trails and operational summaries                                   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Detailed Workflow Phases

### Phase 1: Data Collection

**Purpose**: Collects and processes manufacturing data from various sources.

**Components**:
- DataPipelineOrchestrator
- Update Detection System

**Process Flow**:
1. Initialize DataPipelineOrchestrator with configuration parameters
2. Execute data collection pipeline
3. Generate data pipeline report
4. Detect changes in collected data
5. Trigger subsequent phases based on detected changes

**Directory Structure**:
```
agents/database
â”œâ”€â”€ databaseSchemas.json                                     # Source directory (Data Loader)
â””â”€â”€ dynamicDatabase/                                         # Source directory (Data Collection)
    â”œâ”€â”€ monthlyReports_history/                              # Product records source (Data Collection)
    â”‚   â””â”€â”€ monthlyReports_YYYYMM.xlsb
    â””â”€â”€ purchaseOrders_history/                              # Purchase orders source (Data Collection)
        â””â”€â”€ purchaseOrder_YYYYMM.xlsx

agents/shared_db
â”œâ”€â”€ dynamicDatabase/                                         # Output directory (Data Collection)
|    â”œâ”€â”€ productRecords.parquet                              # Processed product records (Data Collection)
|    â””â”€â”€ purchaseOrders.parquet                              # Processed purchase orders (Data Collection)
â””â”€â”€ DataLoaderAgent/                                         # Output directory (Data Collection)
     â”œâ”€â”€ historical_db/                                      # store historical versions
     â”œâ”€â”€ newest/                                             # store newest versions
     |    â”œâ”€â”€ YYYYMMDD_HHMM_itemCompositionSummary.parquet   # Processed item composition (Data Loader)
     |    â”œâ”€â”€ YYYYMMDD_HHMM_itemInfo.parquet                 # Processed item information (Data Loader)
     |    â”œâ”€â”€ YYYYMMDD_HHMM_machineInfo.parquet              # Processed machine information (Data Loader)
     |    â”œâ”€â”€ YYYYMMDD_HHMM_moldInfo.parquet                 # Processed mold information (Data Loader)
     |    â”œâ”€â”€ YYYYMMDD_HHMM_moldSpecificationSummary.parquet # Processed mold specifications (Data Loader)
     |    â”œâ”€â”€ YYYYMMDD_HHMM_productRecords.parquet           # Processed product records (Data Loader)
     |    â”œâ”€â”€ YYYYMMDD_HHMM_purchaseOrders.parquet           # Processed purchase orders (Data Loader)
     |    â”œâ”€â”€ YYYYMMDD_HHMM_resinInfo.parquet                # Processed resin information (Data Loader)
     |    â””â”€â”€ path_annotations.json                          # File path annotations (Data Loader)
     â””â”€â”€ change_log.txt                                      # Change tracking log (Data Loader)
```

**Key Methods**:
- `run_data_collection()`: Executes the data collection process
- `detect_updates()`: Analyzes pipeline reports for changes

**Data Flow**: `Raw Data` â†’ `DataPipelineOrchestrator` â†’ `Processed Parquet Files` â†’ `Change Detection`

### Phase 2: Shared Database Building

**Purpose**: Validates collected data and builds shared databases for operational use.

**Components**:
- ValidationOrchestrator: Ensures data integrity
- OrderProgressTracker: Monitors manufacturing progress
- ProducingProcessor: Processes production data

**Process Flow**:
1. Check for detected updates from Phase 1
2. If updates found:
   - Run ValidationOrchestrator
   - Execute OrderProgressTracker
   - Process ProducingProcessor
3. If no updates: Skip processing (efficiency optimization)

**Directory Structure**:
```
agents                                 # Source directory
â”œâ”€â”€ database
|    â”œâ”€â”€ databaseSchemas.json          # Schema definitions (ValidationOrchestrator, OrderProgressTracker, ProducingProcessor)
|    â””â”€â”€ sharedDatabaseSchemas.json    # Shared schema definitions (ProducingProcessor)
â””â”€â”€ shared_db                  
    â”œâ”€â”€ dynamicDatabase/                                         
    |    â””â”€â”€ ...                                                       
    â””â”€â”€ DataLoaderAgent/                                      
        â”œâ”€â”€ historical_db/                                      
        â”œâ”€â”€ newest/                                             
        |   â”œâ”€â”€ ...          
        |   â””â”€â”€ path_annotations.json  # File path annotations (ValidationOrchestrator, OrderProgressTracker, ProducingProcessor)                     
        â””â”€â”€ change_log.txt                          
          
agents/shared_db                                              # Output directory
â”œâ”€â”€ ValidationOrchestrator
|     â”œâ”€â”€ historical_db/                                      # Historical validation results                                   
|     â”œâ”€â”€ newest/                                             # Latest validation results                                          
|     |    â””â”€â”€  YYYYMMDD_HHMM_validation_orchestrator.xlsx    # Validation report (ValidationOrchestrator)
|     â””â”€â”€ change_log.txt                                      # Validation changes (OrderProgressTracker source) 
â”œâ”€â”€ OrderProgressTracker
|     â”œâ”€â”€ historical_db/                                      # Historical progress data                                   
|     â”œâ”€â”€ newest/                                             # Latest progress data                                          
|     |    â””â”€â”€  YYYYMMDD_HHMM_auto_status.xlsx                # Progress status report (OrderProgressTracker)
|     â””â”€â”€ change_log.txt                                      # Progress changes (ProducingProcessor source)  
â”œâ”€â”€ MoldStabilityIndexCalculator/ 
|    â”œâ”€â”€ historical_db/                                      
|    â”œâ”€â”€ newest/ 
|    |   â””â”€â”€ YYYYMMDD_HHMM_mold_stability_index.xlsx
|    â””â”€â”€ change_log.txt                                       # Stability index changes (ProducingProcessor, MoldMachineFeatureWeightCalculator source)  
â”œâ”€â”€ MoldMachineFeatureWeightCalculator/ 
|   â”œâ”€â”€ historical_db/                                      
|   â”œâ”€â”€ newest/ 
|   |   â””â”€â”€ YYYYMMDD_HHMM_confidence_report.txt
|   â”œâ”€â”€ change_log.txt      
|   â””â”€â”€ weights_hist.xlsx                                     # Machine weight history (ProducingProcessor source)   
â””â”€â”€ ProducingProcessor
      â”œâ”€â”€ historical_db/                                      # Historical production data                                   
      â”œâ”€â”€ newest/                                             # Latest production data                                          
      |    â””â”€â”€  YYYYMMDD_HHMM_producing_processor.xlsx        # Production analysis report (ProducingProcessor)
      â””â”€â”€ change_log.txt                                      # Production changes
```

**Key Methods**:
- `run_shared_db_building()`: Coordinates shared database building
- `run_validation_orchestrator()`: Executes data validation
- `run_order_progress_tracker()`: Tracks order progress
- `run_producing_processor()`: Processes production data

**Data Flow**: `Processed Data` â†’ `Validation/Progress/Production Agents` â†’ `Analysis Reports` â†’ `Change Logs`

### Phase 2.5: Historical Insights Generation (Conditional)

**Purpose**: Generates historical insights and analytical data when sufficient historical records are available.

**Trigger Condition**: Executes when the number of unique record dates is divisible by `historical_insight_threshold` (default: 30).

**Components**:
- MoldStabilityIndexCalculator: Calculates mold stability indices
- MoldMachineFeatureWeightCalculator: Calculates machine feature weights

**Process Flow**:
```
Check Historical Records
         â†“
[Records % threshold == 0] ?
         â†“ YES                    â†“ NO
Run Stability Calculator    Skip Historical Insights
         â†“
Run Weight Calculator
         â†“
Update Historical Database
```

**Benefits**:
- Provides data-driven insights for production optimization
- Enables predictive maintenance scheduling
- Improves mold-machine matching efficiency
- Supports long-term capacity planning

### Phase 3: Initial Planning

**Purpose**: Performs production planning and optimization based on current data state.

**Components**:
- PendingProcessor: Handles pending order processing

**Process Flow**:
1. Check for purchase order changes
2. If changes detected:
   - Initialize PendingProcessor with configuration
   - Execute order processing and optimization
3. Generate planning reports

**Directory Structure**:
```
agents                                                    # Source directory
â”œâ”€â”€ database
|    â”œâ”€â”€ databaseSchemas.json                             # Schema definitions (PendingProcessor)              
|    â””â”€â”€ sharedDatabaseSchemas.json                       # Shared schema definitions (PendingProcessor)
â””â”€â”€ shared_db                  
    â”œâ”€â”€ dynamicDatabase/                                  # Dynamic data source                                                       
    |   â””â”€â”€ ...                           
    â”œâ”€â”€ DataLoaderAgent/                                  # Loaded data source                              
    |   â”œâ”€â”€ historical_db/                                      
    |   â”œâ”€â”€ newest/                                              
    |   |   â”œâ”€â”€ ...          
    |   |   â””â”€â”€ path_annotations.json                     # File path annotations (PendingProcessor source)                  
    |   â””â”€â”€ change_log.txt             
    â”œâ”€â”€ ValidationOrchestrator/                           # Validation results
    |   â””â”€â”€ ...   
    â”œâ”€â”€ OrderProgressTracker/
    |   â””â”€â”€ ...       
    â”œâ”€â”€ MoldStabilityIndexCalculator/                     # Stability calculations                
    |   â””â”€â”€ ...                   
    â”œâ”€â”€ MoldMachineFeatureWeightCalculator/               # Weight calculations               
    |   â””â”€â”€ ...                    
    â””â”€â”€ ProducingProcessor                                # Production analysis
        â”œâ”€â”€ historical_db/                                      
        â”œâ”€â”€ newest/                                             
        |   â””â”€â”€ YYYYMMDD_HHMM_producing_processor.xlsx        
        â””â”€â”€ change_log.txt                                # Production changes (PendingProcessor source)

agents/shared_db/PendingProcessor                         # Output directory
    â”œâ”€â”€ historical_db/                                    # Historical planning data                               
    â”œâ”€â”€ newest/                                           # Latest planning data                                        
    |    â””â”€â”€  YYYYMMDD_HHMM_PendingProcessor.xlsx         # Planning optimization report (PendingProcessor)
    â””â”€â”€ change_log.txt                                    # Planning changes
```

**Key Methods**:
- `run_initial_planner()`: Orchestrates initial planning phase
- `run_pending_processor()`: Processes pending orders

**Data Flow**: `Analysis Results` â†’ `PendingProcessor` â†’ `Optimized Planning` â†’ `Final Reports`

## Comprehensive Reporting System

**Purpose**: Generates detailed reports with complete workflow execution summaries and audit trails.

**Directory Structure**:
```
agents/shared_db/OptiMoldIQWorkflow
    â”œâ”€â”€ historical_reports/                               # Historical workflow reports                                   
    â”œâ”€â”€ latest/                                           # Latest workflow reports                                          
    |    â””â”€â”€ YYYYMMDD_HHMM_OptiMoldIQWorkflow_report.txt  # Workflow execution summary (OptiMoldIQWorkflow)
    â””â”€â”€ change_log.txt                                    # Workflow execution log
```

**Data Flow**: `All Phases` â†’ `Comprehensive Report Generation` â†’ `Timestamped Vietnamese Reports`

## Workflow Execution Logic

### Main Workflow Method

```python
def run_workflow() -> Dict[str, Any]:
    """
    Executes the complete daily workflow with conditional phase processing.
    
    Returns:
        Dict[str, Any]: Complete workflow execution report
    """
    
    # PHASE 1: Always execute data collection
    data_pipeline_report = self.run_data_collection()
    
    # Detect changes to determine if subsequent phases should run
    trigger, updated_db_details, historical_insight_request = self.detect_updates(data_pipeline_report)
    
    # PHASE 2: Conditional shared database building
    shared_db_report = None
    if trigger:
        shared_db_report = self.run_shared_db_building(historical_insight_request)
    
    # PHASE 3: Conditional initial planning
    initial_planner_report = None
    if trigger and self._should_run_initial_planner(updated_db_details):
        initial_planner_report = self.run_initial_planner()
    
    # Generate comprehensive workflow report
    workflow_report = self._generate_workflow_report(
        data_pipeline_report, 
        shared_db_report, 
        initial_planner_report,
        trigger,
        updated_db_details,
        historical_insight_request
    )
    
    return workflow_report
```

### Change Detection Logic

```python
def detect_updates(self, data_pipeline_report) -> Tuple[bool, List[str], bool]:
    """
    Detects updates in data pipeline report to determine phase execution.
    
    Args:
        data_pipeline_report: Report from DataPipelineOrchestrator
        
    Returns:
        tuple: (trigger_flag, updated_databases, historical_insight_flag)
    """
    
    trigger = False
    updated_db_details = []
    historical_insight_request = False
    
    # Check collector results for changes
    if 'DataCollectorAgent' in data_pipeline_report:
        collector_results = data_pipeline_report['DataCollectorAgent']
        # Analyze collector change indicators
        
    # Check loader results for changes  
    if 'DataLoaderAgent' in data_pipeline_report:
        loader_results = data_pipeline_report['DataLoaderAgent']
        # Analyze loader change indicators
        
    # Check for historical insights trigger
    if self._should_generate_historical_insights():
        historical_insight_request = True
        
    return trigger, updated_db_details, historical_insight_request
```

### Historical Insights Trigger Logic

```python
def _should_generate_historical_insights(self) -> bool:
    """
    Determines if historical insights should be generated based on record count.
    
    Returns:
        bool: True if insights should be generated
    """
    
    # Check if productRecords has sufficient historical data
    product_records_path = self.path_manager.get_product_records_path()
    
    if product_records_path.exists():
        # Load and analyze record dates
        df = pd.read_parquet(product_records_path)
        unique_dates = df['date'].nunique() if 'date' in df.columns else 0
        
        # Trigger when divisible by threshold
        if unique_dates > 0 and unique_dates % self.config.historical_insight_threshold == 0:
            return True
            
    return False
```

### Purchase Order Change Detection

```python
def _should_run_initial_planner(self, updated_db_details: List[str]) -> bool:
    """
    Determines if initial planner should run based on purchase order changes.
    
    Args:
        updated_db_details: List of updated database details
        
    Returns:
        bool: True if planner should execute
    """
    
    # Look for purchase order related changes
    po_keywords = ['purchaseOrders', 'purchase_orders', 'PO']
    
    for detail in updated_db_details:
        for keyword in po_keywords:
            if keyword.lower() in detail.lower():
                return True
                
    return False
```

## Phase Execution Details

### Phase 1: Data Collection Execution

```python
def run_data_collection(self) -> Dict[str, Any]:
    """Execute Phase 1: Data Collection"""
    
    def _execute_data_collection():
        # Initialize DataPipelineOrchestrator
        orchestrator = DataPipelineOrchestrator(
            db_dir=self.config.db_dir,
            dynamic_db_dir=self.config.dynamic_db_dir,
            shared_db_dir=self.config.shared_db_dir
        )
        
        # Execute data pipeline
        report = orchestrator.run_pipeline()
        
        # Log collection results
        logger.info(f"Data collection completed: {len(report)} components processed")
        
        return report
    
    return self._safe_execute("DataCollection", _execute_data_collection)
```

### Phase 2: Shared Database Building Execution

```python
def run_shared_db_building(self, historical_insight_request: bool = False) -> Dict[str, Any]:
    """Execute Phase 2: Shared Database Building"""
    
    def _execute_shared_db_building():
        results = {}
        
        # Execute ValidationOrchestrator
        results['ValidationOrchestrator'] = self.run_validation_orchestrator()
        
        # Execute OrderProgressTracker
        results['OrderProgressTracker'] = self.run_order_progress_tracker()
        
        # Execute Historical Insights (if requested)
        if historical_insight_request:
            results['HistoricalInsights'] = self._run_historical_insights()
        
        # Execute ProducingProcessor
        results['ProducingProcessor'] = self.run_producing_processor()
        
        return results
    
    return self._safe_execute("SharedDBBuilding", _execute_shared_db_building)
```

### Phase 2.5: Historical Insights Execution

```python
def _run_historical_insights(self) -> Dict[str, Any]:
    """Execute Phase 2.5: Historical Insights Generation"""
    
    def _execute_historical_insights():
        results = {}
        
        # Run MoldStabilityIndexCalculator
        stability_calculator = MoldStabilityIndexCalculator(
            cavity_stability_threshold=self.config.cavity_stability_threshold,
            cycle_stability_threshold=self.config.cycle_stability_threshold,
            total_records_threshold=self.config.total_records_threshold
        )
        results['MoldStabilityIndex'] = stability_calculator.calculate()
        
        # Run MoldMachineFeatureWeightCalculator
        weight_calculator = MoldMachineFeatureWeightCalculator(
            scaling=self.config.scaling,
            confidence_weight=self.config.confidence_weight,
            n_bootstrap=self.config.n_bootstrap,
            confidence_level=self.config.confidence_level,
            min_sample_size=self.config.min_sample_size,
            targets=self.config.targets
        )
        results['MoldMachineWeights'] = weight_calculator.calculate()
        
        return results
    
    return self._safe_execute("HistoricalInsights", _execute_historical_insights)
```

### Phase 3: Initial Planning Execution

```python
def run_initial_planner(self) -> Dict[str, Any]:
    """Execute Phase 3: Initial Planning"""
    
    def _execute_initial_planner():
        # Initialize PendingProcessor
        processor = PendingProcessor(
            max_load_threshold=self.config.max_load_threshold,
            priority_order=self.config.priority_order,
            verbose=self.config.verbose,
            use_sample_data=self.config.use_sample_data
        )
        
        # Execute pending order processing
        results = processor.process_pending_orders()
        
        # Generate optimization reports
        optimization_report = processor.generate_optimization_report()
        
        return {
            'processing_results': results,
            'optimization_report': optimization_report
        }
    
    return self._safe_execute("InitialPlanner", _execute_initial_planner)
```

## Error Handling and Recovery

### Safe Execution Framework

```python
def _safe_execute(self, operation_name: str, operation_func, *args, **kwargs) -> Any:
    """
    Safely executes workflow operations with comprehensive error handling.
    
    Args:
        operation_name: Human-readable operation name for logging
        operation_func: Function to execute safely
        *args, **kwargs: Arguments to pass to operation_func
        
    Returns:
        Any: Result from operation_func
        
    Raises:
        WorkflowError: Wrapped exception with context
    """
    
    start_time = time.time()
    logger.info(f"Starting {operation_name}")
    
    try:
        result = operation_func(*args, **kwargs)
        execution_time = time.time() - start_time
        logger.info(f"Completed {operation_name} in {execution_time:.2f} seconds")
        return result
        
    except Exception as e:
        execution_time = time.time() - start_time
        logger.error(f"Failed {operation_name} after {execution_time:.2f} seconds: {e}")
        raise WorkflowError(f"Failed to execute {operation_name}: {e}") from e
```

### Error Recovery Strategies

1. **Phase Isolation**: Each phase is independent and can continue even if previous phases fail
2. **Graceful Degradation**: System continues with available data when components fail
3. **Comprehensive Logging**: All errors are logged with full context and stack traces
4. **Report Integration**: Error details are included in final workflow reports

### Monitoring and Alerting

```python
def _monitor_workflow_health(self) -> Dict[str, Any]:
    """
    Monitors workflow health and generates alerts for critical issues.
    
    Returns:
        Dict[str, Any]: Health status and alerts
    """
    
    health_status = {
        'status': 'healthy',
        'alerts': [],
        'warnings': [],
        'metrics': {}
    }
    
    # Check disk space
    if self._check_disk_space() < 0.1:  # Less than 10% free
        health_status['alerts'].append("Low disk space detected")
        health_status['status'] = 'critical'
    
    # Check database connectivity
    if not self._check_database_connectivity():
        health_status['alerts'].append("Database connectivity issues")
        health_status['status'] = 'critical'
    
    # Check recent execution times
    recent_times = self._get_recent_execution_times()
    if max(recent_times) > self.config.max_execution_time:
        health_status['warnings'].append("Long execution times detected")
    
    return health_status
```

## Performance Optimization

### Conditional Processing Benefits

- **Resource Efficiency**: Only processes when changes are detected
- **Time Optimization**: Skips unnecessary operations when no updates exist
- **Cost Reduction**: Minimizes computational overhead for routine checks
- **Scalability**: Maintains performance as data volume grows

### Execution Time Monitoring

```python
def _track_execution_metrics(self, phase_name: str, execution_time: float, success: bool):
    """
    Tracks execution metrics for performance analysis.
    
    Args:
        phase_name: Name of executed phase
        execution_time: Time taken in seconds
        success: Whether execution was successful
    """
    
    metrics = {
        'phase': phase_name,
        'execution_time': execution_time,
        'success': success,
        'timestamp': datetime.now().isoformat(),
        'memory_usage': self._get_memory_usage(),
        'cpu_usage': self._get_cpu_usage()
    }
    
    # Store metrics for analysis
    self._store_performance_metrics(metrics)
```

### Optimization Recommendations

1. **Database Indexing**: Ensure proper indexing on frequently queried columns
2. **Parallel Processing**: Consider parallel execution for independent agents
3. **Caching Strategy**: Implement caching for frequently accessed data
4. **Resource Monitoring**: Monitor CPU, memory, and disk usage during execution
5. **Data Partitioning**: Partition large datasets by date or other relevant dimensions